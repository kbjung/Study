{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('GHKGSGNYRWRCKNQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK'\n",
    "query_sequence = \"\".join(query_sequence.split())\n",
    "query_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobname = 'test'\n",
    "basejobname = \"\".join(jobname.split())\n",
    "basejobname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basejobname = re.sub(r'\\W+', '', basejobname)\n",
    "basejobname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_a5e17'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_hash(x,y):\n",
    "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
    "jobname = add_hash(basejobname, query_sequence)\n",
    "jobname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### Advanced settings\n",
    "model_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\", \"alphafold2\"]\n",
    "#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
    "#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
    "num_recycles = \"3\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
    "#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
    "recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
    "#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
    "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
    "#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
    "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
    "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
    "calc_extra_ptm = False #@param {type:\"boolean\"}\n",
    "#@markdown - return pairwise chain iptm/actifptm\n",
    "\n",
    "#@markdown #### Sample settings\n",
    "#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
    "#@markdown -  decrease `max_msa` to increase uncertainity\n",
    "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
    "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
    "use_dropout = False #@param {type:\"boolean\"}\n",
    "\n",
    "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
    "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
    "if max_msa == \"auto\": max_msa = None\n",
    "\n",
    "#@markdown #### Save settings\n",
    "save_all = False #@param {type:\"boolean\"}\n",
    "save_recycles = False #@param {type:\"boolean\"}\n",
    "save_to_google_drive = False #@param {type:\"boolean\"}\n",
    "#@markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
    "dpi = 200 #@param {type:\"integer\"}\n",
    "#@markdown - set dpi for image resolution\n",
    "\n",
    "if save_to_google_drive:\n",
    "  from pydrive2.drive import GoogleDrive\n",
    "  from pydrive2.auth import GoogleAuth\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "  print(\"You are logged into Google Drive and are good to go!\")\n",
    "\n",
    "#@markdown Don't forget to hit `Runtime` -> `Run all` after updating the form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "ENV = {\"TF_FORCE_UNIFIED_MEMORY\":\"1\", \"XLA_PYTHON_CLIENT_MEM_FRACTION\":\"4.0\"}\n",
    "for k,v in ENV.items():\n",
    "    if k not in os.environ: os.environ[k] = v\n",
    "\n",
    "import warnings\n",
    "from Bio import BiopythonDeprecationWarning # what can possibly go wrong...\n",
    "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, TYPE_CHECKING\n",
    "from io import StringIO\n",
    "\n",
    "import importlib_metadata\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import alphafold\n",
    "except ModuleNotFoundError:\n",
    "    raise RuntimeError(\n",
    "        \"\\n\\nalphafold is not installed. Please run `pip install colabfold[alphafold]`\\n\"\n",
    "    )\n",
    "\n",
    "from alphafold.common import protein, residue_constants\n",
    "\n",
    "# delay imports of tensorflow, jax and numpy\n",
    "# loading these for type checking only can take around 10 seconds just to show a CLI usage message\n",
    "if TYPE_CHECKING:\n",
    "    import haiku\n",
    "    from alphafold.model import model\n",
    "    from numpy import ndarray\n",
    "\n",
    "from alphafold.common.protein import Protein\n",
    "from alphafold.data import (\n",
    "    feature_processing,\n",
    "    msa_pairing,\n",
    "    pipeline,\n",
    "    pipeline_multimer,\n",
    "    templates,\n",
    ")\n",
    "from alphafold.data.tools import hhsearch\n",
    "from colabfold.citations import write_bibtex\n",
    "from colabfold.download import default_data_dir, download_alphafold_params\n",
    "from colabfold.utils import (\n",
    "    ACCEPT_DEFAULT_TERMS,\n",
    "    DEFAULT_API_SERVER,\n",
    "    NO_GPU_FOUND,\n",
    "    CIF_REVISION_DATE,\n",
    "    get_commit,\n",
    "    setup_logging,\n",
    "    CFMMCIFIO,\n",
    "    AF3Utils,\n",
    ")\n",
    "from colabfold.input import (\n",
    "    pair_msa,\n",
    "    msa_to_str,\n",
    "    get_queries,\n",
    "    safe_filename,\n",
    "    modified_mapping,\n",
    "    pdb_to_string,\n",
    ")\n",
    "from colabfold.relax import relax_me\n",
    "from colabfold.alphafold import extra_ptm\n",
    "\n",
    "from Bio.PDB import MMCIFParser, PDBParser, MMCIF2Dict\n",
    "from Bio.PDB.PDBIO import Select\n",
    "\n",
    "# logging settings\n",
    "logger = logging.getLogger(__name__)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# from jax 0.4.6, jax._src.lib.xla_bridge moved to jax._src.xla_bridge\n",
    "# suppress warnings: Unable to initialize backend 'rocm' or 'tpu'\n",
    "logging.getLogger('jax._src.xla_bridge').addFilter(lambda _: False) # jax >=0.4.6\n",
    "logging.getLogger('jax._src.lib.xla_bridge').addFilter(lambda _: False) # jax < 0.4.5\n",
    "\n",
    "def mk_mock_template(\n",
    "    query_sequence: Union[List[str], str], num_temp: int = 1\n",
    ") -> Dict[str, Any]:\n",
    "    ln = (\n",
    "        len(query_sequence)\n",
    "        if isinstance(query_sequence, str)\n",
    "        else sum(len(s) for s in query_sequence)\n",
    "    )\n",
    "    output_templates_sequence = \"A\" * ln\n",
    "    output_confidence_scores = np.full(ln, 1.0)\n",
    "\n",
    "    templates_all_atom_positions = np.zeros(\n",
    "        (ln, templates.residue_constants.atom_type_num, 3)\n",
    "    )\n",
    "    templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
    "    templates_aatype = templates.residue_constants.sequence_to_onehot(\n",
    "        output_templates_sequence, templates.residue_constants.HHBLITS_AA_TO_ID\n",
    "    )\n",
    "    template_features = {\n",
    "        \"template_all_atom_positions\": np.tile(\n",
    "            templates_all_atom_positions[None], [num_temp, 1, 1, 1]\n",
    "        ),\n",
    "        \"template_all_atom_masks\": np.tile(\n",
    "            templates_all_atom_masks[None], [num_temp, 1, 1]\n",
    "        ),\n",
    "        \"template_sequence\": [f\"none\".encode()] * num_temp,\n",
    "        \"template_aatype\": np.tile(np.array(templates_aatype)[None], [num_temp, 1, 1]),\n",
    "        \"template_confidence_scores\": np.tile(\n",
    "            output_confidence_scores[None], [num_temp, 1]\n",
    "        ),\n",
    "        \"template_domain_names\": [f\"none\".encode()] * num_temp,\n",
    "        \"template_release_date\": [f\"none\".encode()] * num_temp,\n",
    "        \"template_sum_probs\": np.zeros([num_temp], dtype=np.float32),\n",
    "    }\n",
    "    return template_features\n",
    "\n",
    "def mk_template(\n",
    "    a3m_lines: str, template_path: str, query_sequence: str\n",
    ") -> Dict[str, Any]:\n",
    "    template_featurizer = templates.HhsearchHitFeaturizer(\n",
    "        mmcif_dir=template_path,\n",
    "        max_template_date=\"2100-01-01\",\n",
    "        max_hits=20,\n",
    "        kalign_binary_path=\"kalign\",\n",
    "        release_dates_path=None,\n",
    "        obsolete_pdbs_path=None,\n",
    "    )\n",
    "\n",
    "    hhsearch_pdb70_runner = hhsearch.HHSearch(\n",
    "        binary_path=\"hhsearch\", databases=[f\"{template_path}/pdb70\"]\n",
    "    )\n",
    "\n",
    "    hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
    "    hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
    "    templates_result = template_featurizer.get_templates(\n",
    "        query_sequence=query_sequence, hits=hhsearch_hits\n",
    "    )\n",
    "    return dict(templates_result.features)\n",
    "\n",
    "def validate_and_fix_mmcif(cif_file: Path):\n",
    "    \"\"\"validate presence of _entity_poly_seq in cif file and add revision_date if missing\"\"\"\n",
    "    # check that required poly_seq and revision_date fields are present\n",
    "    cif_dict = MMCIF2Dict.MMCIF2Dict(cif_file)\n",
    "    required = [\n",
    "        \"_chem_comp.id\",\n",
    "        \"_chem_comp.type\",\n",
    "        \"_struct_asym.id\",\n",
    "        \"_struct_asym.entity_id\",\n",
    "        \"_entity_poly_seq.mon_id\",\n",
    "    ]\n",
    "    for r in required:\n",
    "        if r not in cif_dict:\n",
    "            raise ValueError(f\"mmCIF file {cif_file} is missing required field {r}.\")\n",
    "    if \"_pdbx_audit_revision_history.revision_date\" not in cif_dict:\n",
    "        logger.info(\n",
    "            f\"Adding missing field revision_date to {cif_file}. Backing up original file to {cif_file}.bak.\"\n",
    "        )\n",
    "        shutil.copy2(cif_file, str(cif_file) + \".bak\")\n",
    "        with open(cif_file, \"a\") as f:\n",
    "            f.write(CIF_REVISION_DATE)\n",
    "\n",
    "class ReplaceOrRemoveHetatmSelect(Select):\n",
    "  def accept_residue(self, residue):\n",
    "    hetfield, _, _ = residue.get_id()\n",
    "    if hetfield != \" \":\n",
    "      if residue.resname in modified_mapping:\n",
    "        # set unmodified resname\n",
    "        residue.resname = modified_mapping[residue.resname]\n",
    "        # clear hetatm flag\n",
    "        residue._id = (\" \", residue._id[1], \" \")\n",
    "        t = residue.full_id\n",
    "        residue.full_id = (t[0], t[1], t[2], residue._id)\n",
    "        return 1\n",
    "      return 0\n",
    "    else:\n",
    "      return 1\n",
    "\n",
    "def convert_pdb_to_mmcif(pdb_file: Path):\n",
    "    \"\"\"convert existing pdb files into mmcif with the required poly_seq and revision_date\"\"\"\n",
    "    i = pdb_file.stem\n",
    "    cif_file = pdb_file.parent.joinpath(f\"{i}.cif\")\n",
    "    if cif_file.is_file():\n",
    "        return\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(i, pdb_file)\n",
    "    cif_io = CFMMCIFIO()\n",
    "    cif_io.set_structure(structure)\n",
    "    cif_io.save(str(cif_file), ReplaceOrRemoveHetatmSelect())\n",
    "\n",
    "def mk_hhsearch_db(template_dir: str):\n",
    "    template_path = Path(template_dir)\n",
    "\n",
    "    cif_files = template_path.glob(\"*.cif\")\n",
    "    for cif_file in cif_files:\n",
    "        validate_and_fix_mmcif(cif_file)\n",
    "\n",
    "    pdb_files = template_path.glob(\"*.pdb\")\n",
    "    for pdb_file in pdb_files:\n",
    "        convert_pdb_to_mmcif(pdb_file)\n",
    "\n",
    "    pdb70_db_files = template_path.glob(\"pdb70*\")\n",
    "    for f in pdb70_db_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    with open(template_path.joinpath(\"pdb70_a3m.ffdata\"), \"w\") as a3m, open(\n",
    "        template_path.joinpath(\"pdb70_cs219.ffindex\"), \"w\"\n",
    "    ) as cs219_index, open(\n",
    "        template_path.joinpath(\"pdb70_a3m.ffindex\"), \"w\"\n",
    "    ) as a3m_index, open(\n",
    "        template_path.joinpath(\"pdb70_cs219.ffdata\"), \"w\"\n",
    "    ) as cs219:\n",
    "        n = 1000000\n",
    "        index_offset = 0\n",
    "        cif_files = template_path.glob(\"*.cif\")\n",
    "        for cif_file in cif_files:\n",
    "            with open(cif_file) as f:\n",
    "                cif_string = f.read()\n",
    "            cif_fh = StringIO(cif_string)\n",
    "            parser = MMCIFParser(QUIET=True)\n",
    "            structure = parser.get_structure(\"none\", cif_fh)\n",
    "            models = list(structure.get_models())\n",
    "            if len(models) != 1:\n",
    "                logger.warning(f\"WARNING: Found {len(models)} models in {cif_file}. The first model will be used as a template.\", )\n",
    "                # raise ValueError(\n",
    "                #     f\"Only single model PDBs are supported. Found {len(models)} models in {cif_file}.\"\n",
    "                # )\n",
    "            model = models[0]\n",
    "            for chain in model:\n",
    "                amino_acid_res = []\n",
    "                for res in chain:\n",
    "                    if res.id[2] != \" \":\n",
    "                        logger.warning(f\"WARNING: Found insertion code at chain {chain.id} and residue index {res.id[1]} of {cif_file}. \"\n",
    "                                       \"This file cannot be used as a template.\")\n",
    "                        continue\n",
    "                        # raise ValueError(\n",
    "                        #     f\"PDB {cif_file} contains an insertion code at chain {chain.id} and residue \"\n",
    "                        #     f\"index {res.id[1]}. These are not supported.\"\n",
    "                        # )\n",
    "                    amino_acid_res.append(\n",
    "                        residue_constants.restype_3to1.get(res.resname, \"X\")\n",
    "                    )\n",
    "\n",
    "                protein_str = \"\".join(amino_acid_res)\n",
    "                a3m_str = f\">{cif_file.stem}_{chain.id}\\n{protein_str}\\n\\0\"\n",
    "                a3m_str_len = len(a3m_str)\n",
    "                a3m_index.write(f\"{n}\\t{index_offset}\\t{a3m_str_len}\\n\")\n",
    "                cs219_index.write(f\"{n}\\t{index_offset}\\t{len(protein_str)}\\n\")\n",
    "                index_offset += a3m_str_len\n",
    "                a3m.write(a3m_str)\n",
    "                cs219.write(\"\\n\\0\")\n",
    "                n += 1\n",
    "\n",
    "def pad_input(\n",
    "    input_features: model.features.FeatureDict,\n",
    "    model_runner: model.RunModel,\n",
    "    model_name: str,\n",
    "    pad_len: int,\n",
    "    use_templates: bool,\n",
    ") -> model.features.FeatureDict:\n",
    "    from colabfold.alphafold.msa import make_fixed_size\n",
    "\n",
    "    model_config = model_runner.config\n",
    "    eval_cfg = model_config.data.eval\n",
    "    crop_feats = {k: [None] + v for k, v in dict(eval_cfg.feat).items()}\n",
    "\n",
    "    max_msa_clusters = eval_cfg.max_msa_clusters\n",
    "    max_extra_msa = model_config.data.common.max_extra_msa\n",
    "    # templates models\n",
    "    if (model_name == \"model_1\" or model_name == \"model_2\") and use_templates:\n",
    "        pad_msa_clusters = max_msa_clusters - eval_cfg.max_templates\n",
    "    else:\n",
    "        pad_msa_clusters = max_msa_clusters\n",
    "\n",
    "    max_msa_clusters = pad_msa_clusters\n",
    "\n",
    "    # let's try pad (num_res + X)\n",
    "    input_fix = make_fixed_size(\n",
    "        input_features,\n",
    "        crop_feats,\n",
    "        msa_cluster_size=max_msa_clusters,  # true_msa (4, 512, 68)\n",
    "        extra_msa_size=max_extra_msa,  # extra_msa (4, 5120, 68)\n",
    "        num_res=pad_len,  # aatype (4, 68)\n",
    "        num_templates=4,\n",
    "    )  # template_mask (4, 4) second value\n",
    "    return input_fix\n",
    "\n",
    "class file_manager:\n",
    "    def __init__(self, prefix: str, result_dir: Path):\n",
    "        self.prefix = prefix\n",
    "        self.result_dir = result_dir\n",
    "        self.tag = None\n",
    "        self.files = {}\n",
    "\n",
    "    def get(self, x: str, ext:str) -> Path:\n",
    "        if self.tag not in self.files:\n",
    "            self.files[self.tag] = []\n",
    "        file = self.result_dir.joinpath(f\"{self.prefix}_{x}_{self.tag}.{ext}\")\n",
    "        self.files[self.tag].append([x,ext,file])\n",
    "        return file\n",
    "\n",
    "    def set_tag(self, tag):\n",
    "        self.tag = tag\n",
    "\n",
    "def predict_structure(\n",
    "    prefix: str,\n",
    "    result_dir: Path,\n",
    "    feature_dict: Dict[str, Any],\n",
    "    is_complex: bool,\n",
    "    use_templates: bool,\n",
    "    sequences_lengths: List[int],\n",
    "    pad_len: int,\n",
    "    model_type: str,\n",
    "    model_runner_and_params: List[Tuple[str, model.RunModel, haiku.Params]],\n",
    "    initial_guess: str = None,\n",
    "    num_relax: int = 0,\n",
    "    relax_max_iterations: int = 0,\n",
    "    relax_tolerance: float = 2.39,\n",
    "    relax_stiffness: float = 10.0,\n",
    "    relax_max_outer_iterations: int = 3,\n",
    "    rank_by: str = \"auto\",\n",
    "    random_seed: int = 0,\n",
    "    num_seeds: int = 1,\n",
    "    stop_at_score: float = 100,\n",
    "    prediction_callback: Callable[[Any, Any, Any, Any, Any], Any] = None,\n",
    "    use_gpu_relax: bool = False,\n",
    "    save_all: bool = False,\n",
    "    save_single_representations: bool = False,\n",
    "    save_pair_representations: bool = False,\n",
    "    save_recycles: bool = False,\n",
    "    calc_extra_ptm: bool = False,\n",
    "    use_probs_extra: bool = True,\n",
    "):\n",
    "    \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
    "    mean_scores = []\n",
    "    conf = []\n",
    "    unrelaxed_pdb_lines = []\n",
    "    prediction_times = []\n",
    "    model_names = []\n",
    "    files = file_manager(prefix, result_dir)\n",
    "    seq_len = sum(sequences_lengths)\n",
    "\n",
    "    # iterate through random seeds\n",
    "    for seed_num, seed in enumerate(range(random_seed, random_seed+num_seeds)):\n",
    "\n",
    "        # iterate through models\n",
    "        for model_num, (model_name, model_runner, params) in enumerate(model_runner_and_params):\n",
    "\n",
    "            # swap params to avoid recompiling\n",
    "            model_runner.params = params\n",
    "\n",
    "            #########################\n",
    "            # process input features\n",
    "            #########################\n",
    "            if \"multimer\" in model_type:\n",
    "                if model_num == 0 and seed_num == 0:\n",
    "                    # TODO: add pad_input_mulitmer()\n",
    "                    input_features = feature_dict\n",
    "                    input_features[\"asym_id\"] = input_features[\"asym_id\"] - input_features[\"asym_id\"][...,0]\n",
    "            else:\n",
    "                if model_num == 0:\n",
    "                    input_features = model_runner.process_features(feature_dict, random_seed=seed)\n",
    "                    r = input_features[\"aatype\"].shape[0]\n",
    "                    input_features[\"asym_id\"] = np.tile(feature_dict[\"asym_id\"],r).reshape(r,-1)\n",
    "                    if seq_len < pad_len:\n",
    "                        input_features = pad_input(input_features, model_runner,\n",
    "                            model_name, pad_len, use_templates)\n",
    "                        logger.info(f\"Padding length to {pad_len}\")\n",
    "\n",
    "\n",
    "            tag = f\"{model_type}_{model_name}_seed_{seed:03d}\"\n",
    "            model_names.append(tag)\n",
    "            files.set_tag(tag)\n",
    "\n",
    "            # initial guess\n",
    "            if initial_guess:\n",
    "                input_guess = Path(initial_guess)\n",
    "                if input_guess.suffix == \".pdb\":\n",
    "                    pdb_string = pdb_to_string(initial_guess)\n",
    "                    input_features[\"all_atom_positions\"] = protein.from_pdb_string(pdb_string).atom_positions\n",
    "                elif input_guess.suffix == \".cif\":\n",
    "                    input_features[\"all_atom_positions\"] = protein.from_mmcif_string(input_guess.read_text()).atom_positions\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported initial guess file format: {initial_guess}\")\n",
    "                \n",
    "\n",
    "            ########################\n",
    "            # predict\n",
    "            ########################\n",
    "            start = time.time()\n",
    "\n",
    "            # monitor intermediate results\n",
    "            def callback(result, recycles):\n",
    "                if recycles == 0: result.pop(\"tol\",None)\n",
    "                if not is_complex: result.pop(\"iptm\",None)\n",
    "                print_line = \"\"\n",
    "                for x,y in [[\"mean_plddt\",\"pLDDT\"],[\"ptm\",\"pTM\"],[\"iptm\",\"ipTM\"],[\"tol\",\"tol\"]]:\n",
    "                  if x in result:\n",
    "                    print_line += f\" {y}={result[x]:.3g}\"\n",
    "                logger.info(f\"{tag} recycle={recycles}{print_line}\")\n",
    "\n",
    "                if save_recycles:\n",
    "                    final_atom_mask = result[\"structure_module\"][\"final_atom_mask\"]\n",
    "                    b_factors = result[\"plddt\"][:, None] * final_atom_mask\n",
    "                    unrelaxed_protein = protein.from_prediction(\n",
    "                        features=input_features,\n",
    "                        result=result, b_factors=b_factors,\n",
    "                        remove_leading_feature_dimension=(\"multimer\" not in model_type))\n",
    "                    files.get(\"unrelaxed\",f\"r{recycles}.pdb\").write_text(protein.to_pdb(unrelaxed_protein))\n",
    "\n",
    "                    if save_all:\n",
    "                        with files.get(\"all\",f\"r{recycles}.pickle\").open(\"wb\") as handle:\n",
    "                            pickle.dump(result, handle)\n",
    "                    del unrelaxed_protein\n",
    "\n",
    "            return_representations = save_all or save_single_representations or save_pair_representations\n",
    "\n",
    "            # predict\n",
    "            result, recycles = \\\n",
    "            model_runner.predict(input_features,\n",
    "                random_seed=seed,\n",
    "                return_representations=return_representations,\n",
    "                callback=callback)\n",
    "\n",
    "            if calc_extra_ptm and 'predicted_aligned_error' in result.keys():\n",
    "                extra_ptm_output = extra_ptm.get_chain_and_interface_metrics(result, input_features['asym_id'],\n",
    "                    use_probs_extra=use_probs_extra,\n",
    "                    use_jnp=False)\n",
    "                result.pop('pae_matrix_with_logits', None)\n",
    "                result['actifptm'] = extra_ptm_output['actifptm']\n",
    "            else:\n",
    "                calc_extra_ptm = False\n",
    "            prediction_times.append(time.time() - start)\n",
    "\n",
    "            ########################\n",
    "            # parse results\n",
    "            ########################\n",
    "\n",
    "            # summary metrics\n",
    "            mean_scores.append(result[\"ranking_confidence\"])\n",
    "            if recycles == 0: result.pop(\"tol\",None)\n",
    "            if not is_complex: result.pop(\"iptm\",None)\n",
    "            print_line = \"\"\n",
    "            conf.append({})\n",
    "            for x,y in [[\"mean_plddt\",\"pLDDT\"],[\"ptm\",\"pTM\"],[\"iptm\",\"ipTM\"], ['actifptm', 'actifpTM']]:\n",
    "              if x in result:\n",
    "                print_line += f\" {y}={result[x]:.3g}\"\n",
    "                conf[-1][x] = float(result[x])\n",
    "            conf[-1][\"print_line\"] = print_line\n",
    "            logger.info(f\"{tag} took {prediction_times[-1]:.1f}s ({recycles} recycles)\")\n",
    "\n",
    "            # create protein object\n",
    "            final_atom_mask = result[\"structure_module\"][\"final_atom_mask\"]\n",
    "            b_factors = result[\"plddt\"][:, None] * final_atom_mask\n",
    "            unrelaxed_protein = protein.from_prediction(\n",
    "                features=input_features,\n",
    "                result=result,\n",
    "                b_factors=b_factors,\n",
    "                remove_leading_feature_dimension=(\"multimer\" not in model_type))\n",
    "\n",
    "            # callback for visualization\n",
    "            if prediction_callback is not None:\n",
    "                prediction_callback(unrelaxed_protein, sequences_lengths,\n",
    "                                    result, input_features, (tag, False))\n",
    "\n",
    "            #########################\n",
    "            # save results\n",
    "            #########################\n",
    "\n",
    "            # save pdb\n",
    "            protein_lines = protein.to_pdb(unrelaxed_protein)\n",
    "            files.get(\"unrelaxed\",\"pdb\").write_text(protein_lines)\n",
    "            unrelaxed_pdb_lines.append(protein_lines)\n",
    "\n",
    "            # save raw outputs\n",
    "            if save_all:\n",
    "                with files.get(\"all\",\"pickle\").open(\"wb\") as handle:\n",
    "                    pickle.dump(result, handle)\n",
    "            if save_single_representations:\n",
    "                np.save(files.get(\"single_repr\",\"npy\"),result[\"representations\"][\"single\"])\n",
    "            if save_pair_representations:\n",
    "                np.save(files.get(\"pair_repr\",\"npy\"),result[\"representations\"][\"pair\"])\n",
    "\n",
    "            # write an easy-to-use format (pAE and pLDDT)\n",
    "            with files.get(\"scores\",\"json\").open(\"w\") as handle:\n",
    "                plddt = result[\"plddt\"][:seq_len]\n",
    "                scores = {\"plddt\": np.around(plddt.astype(float), 2).tolist()}\n",
    "                if \"predicted_aligned_error\" in result:\n",
    "                  pae = result[\"predicted_aligned_error\"][:seq_len,:seq_len]\n",
    "                  scores.update({\"max_pae\": pae.max().astype(float).item(),\n",
    "                                 \"pae\": np.around(pae.astype(float), 2).tolist()})\n",
    "                  if calc_extra_ptm:\n",
    "                    scores.update(extra_ptm_output)\n",
    "                  for k in [\"ptm\",\"iptm\"]:\n",
    "                    if k in conf[-1]: scores[k] = np.around(conf[-1][k], 2).item()\n",
    "                  del pae\n",
    "                del plddt\n",
    "                json.dump(scores, handle)\n",
    "\n",
    "            del result, unrelaxed_protein\n",
    "\n",
    "            # early stop criteria fulfilled\n",
    "            if mean_scores[-1] > stop_at_score: break\n",
    "\n",
    "        # early stop criteria fulfilled\n",
    "        if mean_scores[-1] > stop_at_score: break\n",
    "\n",
    "        # cleanup\n",
    "        if \"multimer\" not in model_type: del input_features\n",
    "    if \"multimer\" in model_type: del input_features\n",
    "\n",
    "    ###################################################\n",
    "    # rerank models based on predicted confidence\n",
    "    ###################################################\n",
    "\n",
    "    rank, metric = [],[]\n",
    "    result_files = []\n",
    "    logger.info(f\"reranking models by '{rank_by}' metric\")\n",
    "    model_rank = np.array(mean_scores).argsort()[::-1]\n",
    "    for n, key in enumerate(model_rank):\n",
    "        metric.append(conf[key])\n",
    "        tag = model_names[key]\n",
    "        files.set_tag(tag)\n",
    "        # save relaxed pdb\n",
    "        if n < num_relax:\n",
    "            start = time.time()\n",
    "            pdb_lines = relax_me(\n",
    "                pdb_lines=unrelaxed_pdb_lines[key],\n",
    "                max_iterations=relax_max_iterations,\n",
    "                tolerance=relax_tolerance,\n",
    "                stiffness=relax_stiffness,\n",
    "                max_outer_iterations=relax_max_outer_iterations,\n",
    "                use_gpu=use_gpu_relax)\n",
    "            files.get(\"relaxed\",\"pdb\").write_text(pdb_lines)\n",
    "            logger.info(f\"Relaxation took {(time.time() - start):.1f}s\")\n",
    "\n",
    "        # rename files to include rank\n",
    "        new_tag = f\"rank_{(n+1):03d}_{tag}\"\n",
    "        rank.append(new_tag)\n",
    "        logger.info(f\"{new_tag}{metric[-1]['print_line']}\")\n",
    "        for x, ext, file in files.files[tag]:\n",
    "            new_file = result_dir.joinpath(f\"{prefix}_{x}_{new_tag}.{ext}\")\n",
    "            file.rename(new_file)\n",
    "            result_files.append(new_file)\n",
    "\n",
    "    return {\"rank\":rank,\n",
    "            \"metric\":metric,\n",
    "            \"result_files\":result_files}\n",
    "\n",
    "def get_msa_and_templates(\n",
    "    jobname: str,\n",
    "    query_sequences: Union[str, List[str]],\n",
    "    a3m_lines: Optional[List[str]],\n",
    "    result_dir: Path,\n",
    "    msa_mode: str,\n",
    "    use_templates: bool,\n",
    "    custom_template_path: str,\n",
    "    pair_mode: str,\n",
    "    pairing_strategy: str = \"greedy\",\n",
    "    host_url: str = DEFAULT_API_SERVER,\n",
    "    user_agent: str = \"\",\n",
    ") -> Tuple[\n",
    "    Optional[List[str]], Optional[List[str]], List[str], List[int], List[Dict[str, Any]]\n",
    "]:\n",
    "    from colabfold.colabfold import run_mmseqs2\n",
    "\n",
    "    use_env = msa_mode == \"mmseqs2_uniref_env\" or msa_mode == \"mmseqs2_uniref_env_envpair\"\n",
    "    use_envpair = msa_mode == \"mmseqs2_uniref_env_envpair\"\n",
    "    if isinstance(query_sequences, str): query_sequences = [query_sequences]\n",
    "\n",
    "    # remove duplicates before searching\n",
    "    query_seqs_unique = []\n",
    "    for x in query_sequences:\n",
    "        if x not in query_seqs_unique:\n",
    "            query_seqs_unique.append(x)\n",
    "\n",
    "    # determine how many times is each sequence is used\n",
    "    query_seqs_cardinality = [0] * len(query_seqs_unique)\n",
    "    for seq in query_sequences:\n",
    "        seq_idx = query_seqs_unique.index(seq)\n",
    "        query_seqs_cardinality[seq_idx] += 1\n",
    "\n",
    "    # get template features\n",
    "    template_features = []\n",
    "    if use_templates:\n",
    "        # Skip template search when custom_template_path is provided\n",
    "        if custom_template_path is not None:\n",
    "            if msa_mode == \"single_sequence\":\n",
    "                a3m_lines = []\n",
    "                num = 101\n",
    "                for i, seq in enumerate(query_seqs_unique):\n",
    "                    a3m_lines.append(f\">{num + i}\\n{seq}\")\n",
    "\n",
    "            if a3m_lines is None:\n",
    "                a3m_lines_mmseqs2 = run_mmseqs2(\n",
    "                    query_seqs_unique,\n",
    "                    str(result_dir.joinpath(jobname)),\n",
    "                    use_env,\n",
    "                    use_templates=False,\n",
    "                    host_url=host_url,\n",
    "                    user_agent=user_agent,\n",
    "                )\n",
    "            else:\n",
    "                a3m_lines_mmseqs2 = a3m_lines\n",
    "            template_paths = {}\n",
    "            for index in range(0, len(query_seqs_unique)):\n",
    "                template_paths[index] = custom_template_path\n",
    "        else:\n",
    "            a3m_lines_mmseqs2, template_paths = run_mmseqs2(\n",
    "                query_seqs_unique,\n",
    "                str(result_dir.joinpath(jobname)),\n",
    "                use_env,\n",
    "                use_templates=True,\n",
    "                host_url=host_url,\n",
    "                user_agent=user_agent,\n",
    "            )\n",
    "        if template_paths is None:\n",
    "            logger.info(\"No template detected\")\n",
    "            for index in range(0, len(query_seqs_unique)):\n",
    "                template_feature = mk_mock_template(query_seqs_unique[index])\n",
    "                template_features.append(template_feature)\n",
    "        else:\n",
    "            for index in range(0, len(query_seqs_unique)):\n",
    "                if template_paths[index] is not None:\n",
    "                    template_feature = mk_template(\n",
    "                        a3m_lines_mmseqs2[index],\n",
    "                        template_paths[index],\n",
    "                        query_seqs_unique[index],\n",
    "                    )\n",
    "                    if len(template_feature[\"template_domain_names\"]) == 0:\n",
    "                        template_feature = mk_mock_template(query_seqs_unique[index])\n",
    "                        logger.info(f\"Sequence {index} found no templates\")\n",
    "                    else:\n",
    "                        logger.info(\n",
    "                            f\"Sequence {index} found templates: {template_feature['template_domain_names'].astype(str).tolist()}\"\n",
    "                        )\n",
    "                else:\n",
    "                    template_feature = mk_mock_template(query_seqs_unique[index])\n",
    "                    logger.info(f\"Sequence {index} found no templates\")\n",
    "\n",
    "                template_features.append(template_feature)\n",
    "    else:\n",
    "        for index in range(0, len(query_seqs_unique)):\n",
    "            template_feature = mk_mock_template(query_seqs_unique[index])\n",
    "            template_features.append(template_feature)\n",
    "\n",
    "    if len(query_sequences) == 1:\n",
    "        pair_mode = \"none\"\n",
    "\n",
    "    if pair_mode == \"none\" or pair_mode == \"unpaired\" or pair_mode == \"unpaired_paired\":\n",
    "        if msa_mode == \"single_sequence\":\n",
    "            a3m_lines = []\n",
    "            num = 101\n",
    "            for i, seq in enumerate(query_seqs_unique):\n",
    "                a3m_lines.append(f\">{num + i}\\n{seq}\")\n",
    "        else:\n",
    "            # find normal a3ms\n",
    "            a3m_lines = run_mmseqs2(\n",
    "                query_seqs_unique,\n",
    "                str(result_dir.joinpath(jobname)),\n",
    "                use_env,\n",
    "                use_pairing=False,\n",
    "                host_url=host_url,\n",
    "                user_agent=user_agent,\n",
    "            )\n",
    "    else:\n",
    "        a3m_lines = None\n",
    "\n",
    "    if msa_mode != \"single_sequence\" and (\n",
    "        pair_mode == \"paired\" or pair_mode == \"unpaired_paired\"\n",
    "    ):\n",
    "        # find paired a3m if not a homooligomers\n",
    "        if len(query_seqs_unique) > 1:\n",
    "            paired_a3m_lines = run_mmseqs2(\n",
    "                query_seqs_unique,\n",
    "                str(result_dir.joinpath(jobname)),\n",
    "                use_envpair,\n",
    "                use_pairing=True,\n",
    "                pairing_strategy=pairing_strategy,\n",
    "                host_url=host_url,\n",
    "                user_agent=user_agent,\n",
    "            )\n",
    "        else:\n",
    "            # homooligomers\n",
    "            num = 101\n",
    "            paired_a3m_lines = []\n",
    "            for i in range(0, query_seqs_cardinality[0]):\n",
    "                paired_a3m_lines.append(f\">{num+i}\\n{query_seqs_unique[0]}\\n\")\n",
    "    else:\n",
    "        paired_a3m_lines = None\n",
    "\n",
    "    return (\n",
    "        a3m_lines,\n",
    "        paired_a3m_lines,\n",
    "        query_seqs_unique,\n",
    "        query_seqs_cardinality,\n",
    "        template_features,\n",
    "    )\n",
    "\n",
    "def build_monomer_feature(\n",
    "    sequence: str, unpaired_msa: str, template_features: Dict[str, Any]\n",
    "):\n",
    "    msa = pipeline.parsers.parse_a3m(unpaired_msa)\n",
    "    # gather features\n",
    "    return {\n",
    "        **pipeline.make_sequence_features(\n",
    "            sequence=sequence, description=\"none\", num_res=len(sequence)\n",
    "        ),\n",
    "        **pipeline.make_msa_features([msa]),\n",
    "        **template_features,\n",
    "    }\n",
    "\n",
    "def build_multimer_feature(paired_msa: str) -> Dict[str, ndarray]:\n",
    "    parsed_paired_msa = pipeline.parsers.parse_a3m(paired_msa)\n",
    "    return {\n",
    "        f\"{k}_all_seq\": v\n",
    "        for k, v in pipeline.make_msa_features([parsed_paired_msa]).items()\n",
    "    }\n",
    "\n",
    "def process_multimer_features(\n",
    "    features_for_chain: Dict[str, Dict[str, ndarray]],\n",
    "    min_num_seq: int = 512,\n",
    ") -> Dict[str, ndarray]:\n",
    "    all_chain_features = {}\n",
    "    for chain_id, chain_features in features_for_chain.items():\n",
    "        all_chain_features[chain_id] = pipeline_multimer.convert_monomer_features(\n",
    "            chain_features, chain_id\n",
    "        )\n",
    "\n",
    "    all_chain_features = pipeline_multimer.add_assembly_features(all_chain_features)\n",
    "    # np_example = feature_processing.pair_and_merge(\n",
    "    #    all_chain_features=all_chain_features, is_prokaryote=is_prokaryote)\n",
    "    feature_processing.process_unmerged_features(all_chain_features)\n",
    "    np_chains_list = list(all_chain_features.values())\n",
    "    # noinspection PyProtectedMember\n",
    "    pair_msa_sequences = not feature_processing._is_homomer_or_monomer(np_chains_list)\n",
    "    chains = list(np_chains_list)\n",
    "    chain_keys = chains[0].keys()\n",
    "    updated_chains = []\n",
    "    for chain_num, chain in enumerate(chains):\n",
    "        new_chain = {k: v for k, v in chain.items() if \"_all_seq\" not in k}\n",
    "        for feature_name in chain_keys:\n",
    "            if feature_name.endswith(\"_all_seq\"):\n",
    "                feats_padded = msa_pairing.pad_features(\n",
    "                    chain[feature_name], feature_name\n",
    "                )\n",
    "                new_chain[feature_name] = feats_padded\n",
    "        new_chain[\"num_alignments_all_seq\"] = np.asarray(\n",
    "            len(np_chains_list[chain_num][\"msa_all_seq\"])\n",
    "        )\n",
    "        updated_chains.append(new_chain)\n",
    "    np_chains_list = updated_chains\n",
    "    np_chains_list = feature_processing.crop_chains(\n",
    "        np_chains_list,\n",
    "        msa_crop_size=feature_processing.MSA_CROP_SIZE,\n",
    "        pair_msa_sequences=pair_msa_sequences,\n",
    "        max_templates=feature_processing.MAX_TEMPLATES,\n",
    "    )\n",
    "    # merge_chain_features crashes if there are additional features only present in one chain\n",
    "    # remove all features that are not present in all chains\n",
    "    common_features = set([*np_chains_list[0]]).intersection(*np_chains_list)\n",
    "    np_chains_list = [\n",
    "        {key: value for (key, value) in chain.items() if key in common_features}\n",
    "        for chain in np_chains_list\n",
    "    ]\n",
    "    np_example = feature_processing.msa_pairing.merge_chain_features(\n",
    "        np_chains_list=np_chains_list,\n",
    "        pair_msa_sequences=pair_msa_sequences,\n",
    "        max_templates=feature_processing.MAX_TEMPLATES,\n",
    "    )\n",
    "    np_example = feature_processing.process_final(np_example)\n",
    "\n",
    "    # Pad MSA to avoid zero-sized extra_msa.\n",
    "    np_example = pipeline_multimer.pad_msa(np_example, min_num_seq=min_num_seq)\n",
    "    return np_example\n",
    "\n",
    "def generate_input_feature(\n",
    "    query_seqs_unique: List[str],\n",
    "    query_seqs_cardinality: List[int],\n",
    "    unpaired_msa: List[str],\n",
    "    paired_msa: List[str],\n",
    "    template_features: List[Dict[str, Any]],\n",
    "    is_complex: bool,\n",
    "    model_type: str,\n",
    "    max_seq: int,\n",
    ") -> Tuple[Dict[str, Any], Dict[str, str]]:\n",
    "\n",
    "    input_feature = {}\n",
    "    domain_names = {}\n",
    "    if is_complex and \"multimer\" not in model_type:\n",
    "\n",
    "        full_sequence = \"\"\n",
    "        Ls = []\n",
    "        for sequence_index, sequence in enumerate(query_seqs_unique):\n",
    "            for cardinality in range(0, query_seqs_cardinality[sequence_index]):\n",
    "                full_sequence += sequence\n",
    "                Ls.append(len(sequence))\n",
    "\n",
    "        # bugfix\n",
    "        a3m_lines = f\">0\\n{full_sequence}\\n\"\n",
    "        a3m_lines += pair_msa(query_seqs_unique, query_seqs_cardinality, paired_msa, unpaired_msa)\n",
    "\n",
    "        input_feature = build_monomer_feature(full_sequence, a3m_lines, mk_mock_template(full_sequence))\n",
    "        input_feature[\"residue_index\"] = np.concatenate([np.arange(L) for L in Ls])\n",
    "        input_feature[\"asym_id\"] = np.concatenate([np.full(L,n) for n,L in enumerate(Ls)])\n",
    "        if any(\n",
    "            [\n",
    "                template != b\"none\"\n",
    "                for i in template_features\n",
    "                for template in i[\"template_domain_names\"]\n",
    "            ]\n",
    "        ):\n",
    "            logger.warning(\n",
    "                f\"{model_type} complex does not consider templates. Chose multimer model-type for template support.\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        features_for_chain = {}\n",
    "        chain_cnt = 0\n",
    "        # for each unique sequence\n",
    "        for sequence_index, sequence in enumerate(query_seqs_unique):\n",
    "\n",
    "            # get unpaired msa\n",
    "            if unpaired_msa is None:\n",
    "                input_msa = f\">{101 + sequence_index}\\n{sequence}\"\n",
    "            else:\n",
    "                input_msa = unpaired_msa[sequence_index]\n",
    "\n",
    "            feature_dict = build_monomer_feature(\n",
    "                sequence, input_msa, template_features[sequence_index])\n",
    "\n",
    "            if \"multimer\" in model_type:\n",
    "                # get paired msa\n",
    "                if paired_msa is None:\n",
    "                    input_msa = f\">{101 + sequence_index}\\n{sequence}\"\n",
    "                else:\n",
    "                    input_msa = paired_msa[sequence_index]\n",
    "                feature_dict.update(build_multimer_feature(input_msa))\n",
    "\n",
    "            # for each copy\n",
    "            for cardinality in range(0, query_seqs_cardinality[sequence_index]):\n",
    "                features_for_chain[protein.PDB_CHAIN_IDS[chain_cnt]] = feature_dict\n",
    "                chain_cnt += 1\n",
    "\n",
    "        if \"multimer\" in model_type:\n",
    "            # combine features across all chains\n",
    "            input_feature = process_multimer_features(features_for_chain, min_num_seq=max_seq + 4)\n",
    "            domain_names = {\n",
    "                chain: [\n",
    "                    name.decode(\"UTF-8\")\n",
    "                    for name in feature[\"template_domain_names\"]\n",
    "                    if name != b\"none\"\n",
    "                ]\n",
    "                for (chain, feature) in features_for_chain.items()\n",
    "            }\n",
    "        else:\n",
    "            input_feature = features_for_chain[protein.PDB_CHAIN_IDS[0]]\n",
    "            input_feature[\"asym_id\"] = np.zeros(input_feature[\"aatype\"].shape[0],dtype=int)\n",
    "            domain_names = {\n",
    "                protein.PDB_CHAIN_IDS[0]: [\n",
    "                    name.decode(\"UTF-8\")\n",
    "                    for name in input_feature[\"template_domain_names\"]\n",
    "                    if name != b\"none\"\n",
    "                ]\n",
    "            }\n",
    "    return (input_feature, domain_names)\n",
    "\n",
    "def unserialize_msa(\n",
    "    a3m_lines: List[str], query_sequence: Union[List[str], str]\n",
    ") -> Tuple[\n",
    "    Optional[List[str]],\n",
    "    Optional[List[str]],\n",
    "    List[str],\n",
    "    List[int],\n",
    "    List[Dict[str, Any]],\n",
    "]:\n",
    "    a3m_lines = a3m_lines[0].replace(\"\\x00\", \"\").splitlines()\n",
    "    if not a3m_lines[0].startswith(\"#\") or len(a3m_lines[0][1:].split(\"\\t\")) != 2:\n",
    "        assert isinstance(query_sequence, str)\n",
    "        return (\n",
    "            [\"\\n\".join(a3m_lines)],\n",
    "            None,\n",
    "            [query_sequence],\n",
    "            [1],\n",
    "            [mk_mock_template(query_sequence)],\n",
    "        )\n",
    "\n",
    "    if len(a3m_lines) < 3:\n",
    "        raise ValueError(f\"Unknown file format a3m\")\n",
    "    tab_sep_entries = a3m_lines[0][1:].split(\"\\t\")\n",
    "    query_seq_len = tab_sep_entries[0].split(\",\")\n",
    "    query_seq_len = list(map(int, query_seq_len))\n",
    "    query_seqs_cardinality = tab_sep_entries[1].split(\",\")\n",
    "    query_seqs_cardinality = list(map(int, query_seqs_cardinality))\n",
    "    is_homooligomer = (\n",
    "        True if len(query_seq_len) == 1 and query_seqs_cardinality[0] > 1 else False\n",
    "    )\n",
    "    is_single_protein = (\n",
    "        True if len(query_seq_len) == 1 and query_seqs_cardinality[0] == 1 else False\n",
    "    )\n",
    "    query_seqs_unique = []\n",
    "    prev_query_start = 0\n",
    "    # we store the a3m with cardinality of 1\n",
    "    for n, query_len in enumerate(query_seq_len):\n",
    "        query_seqs_unique.append(\n",
    "            a3m_lines[2][prev_query_start : prev_query_start + query_len]\n",
    "        )\n",
    "        prev_query_start += query_len\n",
    "    paired_msa = [\"\"] * len(query_seq_len)\n",
    "    unpaired_msa = [\"\"] * len(query_seq_len)\n",
    "    already_in = dict()\n",
    "    for i in range(1, len(a3m_lines), 2):\n",
    "        header = a3m_lines[i]\n",
    "        seq = a3m_lines[i + 1]\n",
    "        if (header, seq) in already_in:\n",
    "            continue\n",
    "        already_in[(header, seq)] = 1\n",
    "        has_amino_acid = [False] * len(query_seq_len)\n",
    "        seqs_line = []\n",
    "        prev_pos = 0\n",
    "        for n, query_len in enumerate(query_seq_len):\n",
    "            paired_seq = \"\"\n",
    "            curr_seq_len = 0\n",
    "            for pos in range(prev_pos, len(seq)):\n",
    "                if curr_seq_len == query_len:\n",
    "                    prev_pos = pos\n",
    "                    break\n",
    "                paired_seq += seq[pos]\n",
    "                if seq[pos].islower():\n",
    "                    continue\n",
    "                if seq[pos] != \"-\":\n",
    "                    has_amino_acid[n] = True\n",
    "                curr_seq_len += 1\n",
    "            seqs_line.append(paired_seq)\n",
    "\n",
    "        # if sequence is paired add them to output\n",
    "        if (\n",
    "            not is_single_protein\n",
    "            and not is_homooligomer\n",
    "            and sum(has_amino_acid) > 1 # at least 2 sequences are paired\n",
    "        ):\n",
    "            header_no_faster = header.replace(\">\", \"\")\n",
    "            header_no_faster_split = header_no_faster.split(\"\\t\")\n",
    "            for j in range(0, len(seqs_line)):\n",
    "                paired_msa[j] += \">\" + header_no_faster_split[j] + \"\\n\"\n",
    "                paired_msa[j] += seqs_line[j] + \"\\n\"\n",
    "        else:\n",
    "            for j, seq in enumerate(seqs_line):\n",
    "                if has_amino_acid[j]:\n",
    "                    unpaired_msa[j] += header + \"\\n\"\n",
    "                    unpaired_msa[j] += seq + \"\\n\"\n",
    "    if is_homooligomer:\n",
    "        # homooligomers\n",
    "        num = 101\n",
    "        paired_msa = [\"\"] * query_seqs_cardinality[0]\n",
    "        for i in range(0, query_seqs_cardinality[0]):\n",
    "            paired_msa[i] = \">\" + str(num + i) + \"\\n\" + query_seqs_unique[0] + \"\\n\"\n",
    "    if is_single_protein:\n",
    "        paired_msa = None\n",
    "    template_features = []\n",
    "    for query_seq in query_seqs_unique:\n",
    "        template_feature = mk_mock_template(query_seq)\n",
    "        template_features.append(template_feature)\n",
    "\n",
    "    return (\n",
    "        unpaired_msa,\n",
    "        paired_msa,\n",
    "        query_seqs_unique,\n",
    "        query_seqs_cardinality,\n",
    "        template_features,\n",
    "    )\n",
    "\n",
    "def put_mmciffiles_into_resultdir(\n",
    "    pdb_hit_file: Path,\n",
    "    local_pdb_path: Path,\n",
    "    result_dir: Path,\n",
    "    max_num_templates: int = 20,\n",
    "):\n",
    "    \"\"\"Put mmcif files from local_pdb_path into result_dir and unzip them.\n",
    "    max_num_templates is the maximum number of templates to use (default: 20).\n",
    "    Args:\n",
    "        pdb_hit_file (Path): Path to pdb_hit_file\n",
    "        local_pdb_path (Path): Path to local_pdb_path\n",
    "        result_dir (Path): Path to result_dir\n",
    "        max_num_templates (int): Maximum number of templates to use\n",
    "    \"\"\"\n",
    "    pdb_hit_file = Path(pdb_hit_file)\n",
    "    local_pdb_path = Path(local_pdb_path)\n",
    "    result_dir = Path(result_dir)\n",
    "    result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    query_ids = []\n",
    "    with open(pdb_hit_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            query_id = line.split(\"\\t\")[0]\n",
    "            query_ids.append(query_id)\n",
    "            if query_ids.count(query_id) > max_num_templates:\n",
    "                continue\n",
    "            else:\n",
    "                pdb_id = line.split(\"\\t\")[1][0:4]\n",
    "                divided_pdb_id = pdb_id[1:3]\n",
    "                gzipped_divided_mmcif_file = local_pdb_path / divided_pdb_id / (pdb_id + \".cif.gz\")\n",
    "                gzipped_mmcif_file = local_pdb_path / (pdb_id + \".cif.gz\")\n",
    "                unzipped_mmcif_file = local_pdb_path / (pdb_id + \".cif\")\n",
    "                result_file = result_dir / (pdb_id + \".cif\")\n",
    "                possible_files = [gzipped_divided_mmcif_file, gzipped_mmcif_file, unzipped_mmcif_file]\n",
    "                for file in possible_files:\n",
    "                    if file == gzipped_divided_mmcif_file or file == gzipped_mmcif_file:\n",
    "                        if file.exists():\n",
    "                            with gzip.open(file, \"rb\") as f_in:\n",
    "                                with open(result_file, \"wb\") as f_out:\n",
    "                                    shutil.copyfileobj(f_in, f_out)\n",
    "                                    break\n",
    "                    else:\n",
    "                        # unzipped_mmcif_file\n",
    "                        if file.exists():\n",
    "                            shutil.copyfile(file, result_file)\n",
    "                            break\n",
    "                if not result_file.exists():\n",
    "                    print(f\"WARNING: {pdb_id} does not exist in {local_pdb_path}.\")\n",
    "\n",
    "\n",
    "def run(\n",
    "    queries: List[Tuple[str, Union[str, List[str]], Optional[List[str]], Optional[List[Tuple[str, str,int]]]]],\n",
    "    result_dir: Union[str, Path],\n",
    "    num_models: int,\n",
    "    is_complex: bool,\n",
    "    num_recycles: Optional[int] = None,\n",
    "    recycle_early_stop_tolerance: Optional[float] = None,\n",
    "    model_order: List[int] = [1,2,3,4,5],\n",
    "    initial_guess: str = None,\n",
    "    num_ensemble: int = 1,\n",
    "    model_type: str = \"auto\",\n",
    "    msa_mode: str = \"mmseqs2_uniref_env\",\n",
    "    use_templates: bool = False,\n",
    "    custom_template_path: str = None,\n",
    "    num_relax: int = 0,\n",
    "    relax_max_iterations: int = 0,\n",
    "    relax_tolerance: float = 2.39,\n",
    "    relax_stiffness: float = 10.0,\n",
    "    relax_max_outer_iterations: int = 3,\n",
    "    keep_existing_results: bool = True,\n",
    "    rank_by: str = \"auto\",\n",
    "    pair_mode: str = \"unpaired_paired\",\n",
    "    pairing_strategy: str = \"greedy\",\n",
    "    data_dir: Union[str, Path] = default_data_dir,\n",
    "    host_url: str = DEFAULT_API_SERVER,\n",
    "    user_agent: str = \"\",\n",
    "    random_seed: int = 0,\n",
    "    num_seeds: int = 1,\n",
    "    recompile_padding: Union[int, float] = 10,\n",
    "    zip_results: bool = False,\n",
    "    prediction_callback: Callable[[Any, Any, Any, Any, Any], Any] = None,\n",
    "    save_single_representations: bool = False,\n",
    "    save_pair_representations: bool = False,\n",
    "    jobname_prefix: Optional[str] = None,\n",
    "    save_all: bool = False,\n",
    "    save_recycles: bool = False,\n",
    "    use_dropout: bool = False,\n",
    "    use_gpu_relax: bool = False,\n",
    "    stop_at_score: float = 100,\n",
    "    dpi: int = 200,\n",
    "    max_seq: Optional[int] = None,\n",
    "    max_extra_seq: Optional[int] = None,\n",
    "    pdb_hit_file: Optional[Path] = None,\n",
    "    local_pdb_path: Optional[Path] = None,\n",
    "    use_cluster_profile: bool = True,\n",
    "    feature_dict_callback: Callable[[Any], Any] = None,\n",
    "    calc_extra_ptm: bool = False,\n",
    "    use_probs_extra: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    # check what device is available\n",
    "    try:\n",
    "        # check if TPU is available\n",
    "        import jax.tools.colab_tpu\n",
    "        jax.tools.colab_tpu.setup_tpu()\n",
    "        logger.info('Running on TPU')\n",
    "        DEVICE = \"tpu\"\n",
    "        use_gpu_relax = False\n",
    "    except:\n",
    "        if jax.local_devices()[0].platform == 'cpu':\n",
    "            logger.info(\"WARNING: no GPU detected, will be using CPU\")\n",
    "            DEVICE = \"cpu\"\n",
    "            use_gpu_relax = False\n",
    "        else:\n",
    "            import tensorflow as tf\n",
    "            tf.get_logger().setLevel(logging.ERROR)\n",
    "            logger.info('Running on GPU')\n",
    "            DEVICE = \"gpu\"\n",
    "            # disable GPU on tensorflow\n",
    "            tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "    from alphafold.notebooks.notebook_utils import get_pae_json\n",
    "    from colabfold.alphafold.models import load_models_and_params\n",
    "    from colabfold.colabfold import plot_paes, plot_plddts\n",
    "    from colabfold.plot import plot_msa_v2\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    result_dir = Path(result_dir)\n",
    "    result_dir.mkdir(exist_ok=True)\n",
    "    model_type = set_model_type(is_complex, model_type)\n",
    "\n",
    "    # backward-compatibility with old options\n",
    "    old_names = {\"MMseqs2 (UniRef+Environmental)\":\"mmseqs2_uniref_env\",\n",
    "                 \"MMseqs2 (UniRef+Environmental+Env. Pairing)\":\"mmseqs2_uniref_env_envpair\",\n",
    "                 \"MMseqs2 (UniRef only)\":\"mmseqs2_uniref\",\n",
    "                 \"unpaired+paired\":\"unpaired_paired\"}\n",
    "    msa_mode   = old_names.get(msa_mode,msa_mode)\n",
    "    pair_mode  = old_names.get(pair_mode,pair_mode)\n",
    "    feature_dict_callback = kwargs.pop(\"input_features_callback\", feature_dict_callback)\n",
    "    use_dropout           = kwargs.pop(\"training\", use_dropout)\n",
    "    use_fuse              = kwargs.pop(\"use_fuse\", True)\n",
    "    use_bfloat16          = kwargs.pop(\"use_bfloat16\", True)\n",
    "    max_msa               = kwargs.pop(\"max_msa\",None)\n",
    "    if max_msa is not None:\n",
    "        max_seq, max_extra_seq = [int(x) for x in max_msa.split(\":\")]\n",
    "\n",
    "    if kwargs.pop(\"use_amber\", False) and num_relax == 0:\n",
    "        num_relax = num_models * num_seeds\n",
    "\n",
    "    if len(kwargs) > 0:\n",
    "        print(f\"WARNING: the following options are not being used: {kwargs}\")\n",
    "\n",
    "    # decide how to rank outputs\n",
    "    if rank_by == \"auto\":\n",
    "        rank_by = \"multimer\" if is_complex else \"plddt\"\n",
    "    if \"ptm\" not in model_type and \"multimer\" not in model_type:\n",
    "        rank_by = \"plddt\"\n",
    "\n",
    "    # added for actifptm calculation\n",
    "    if not is_complex and calc_extra_ptm:\n",
    "        logger.info(\"Calculating extra pTM is not supported for single chain prediction, skipping it.\")\n",
    "        calc_extra_ptm = False\n",
    "\n",
    "    # get max length\n",
    "    max_len = 0\n",
    "    max_num = 0\n",
    "    for _, query_sequence, _, _ in queries:\n",
    "        N = 1 if isinstance(query_sequence,str) else len(query_sequence)\n",
    "        L = len(\"\".join(query_sequence))\n",
    "        if L > max_len: max_len = L\n",
    "        if N > max_num: max_num = N\n",
    "\n",
    "    # get max sequences\n",
    "    # 512 5120 = alphafold_ptm (models 1,3,4)\n",
    "    # 512 1024 = alphafold_ptm (models 2,5)\n",
    "    # 508 2048 = alphafold-multimer_v3 (models 1,2,3)\n",
    "    # 508 1152 = alphafold-multimer_v3 (models 4,5)\n",
    "    # 252 1152 = alphafold-multimer_v[1,2]\n",
    "\n",
    "    set_if = lambda x,y: y if x is None else x\n",
    "    if model_type in [\"alphafold2_multimer_v1\",\"alphafold2_multimer_v2\"]:\n",
    "        (max_seq, max_extra_seq) = (set_if(max_seq,252), set_if(max_extra_seq,1152))\n",
    "    elif model_type == \"alphafold2_multimer_v3\":\n",
    "        (max_seq, max_extra_seq) = (set_if(max_seq,508), set_if(max_extra_seq,2048))\n",
    "    else:\n",
    "        (max_seq, max_extra_seq) = (set_if(max_seq,512), set_if(max_extra_seq,5120))\n",
    "\n",
    "    if msa_mode == \"single_sequence\":\n",
    "        num_seqs = 1\n",
    "        if is_complex and \"multimer\" not in model_type: num_seqs += max_num\n",
    "        if use_templates: num_seqs += 4\n",
    "        max_seq = min(num_seqs, max_seq)\n",
    "        max_extra_seq = max(min(num_seqs - max_seq, max_extra_seq), 1)\n",
    "\n",
    "    # sort model order\n",
    "    model_order.sort()\n",
    "\n",
    "    # initial guess\n",
    "    if initial_guess is not None:\n",
    "        logger.info(f'Using initial guess: {initial_guess}')\n",
    "\n",
    "    # Record the parameters of this run\n",
    "    config = {\n",
    "        \"num_queries\": len(queries),\n",
    "        \"use_templates\": use_templates,\n",
    "        \"num_relax\": num_relax,\n",
    "        \"relax_max_iterations\": relax_max_iterations,\n",
    "        \"relax_tolerance\": relax_tolerance,\n",
    "        \"relax_stiffness\": relax_stiffness,\n",
    "        \"relax_max_outer_iterations\": relax_max_outer_iterations,\n",
    "        \"msa_mode\": msa_mode,\n",
    "        \"model_type\": model_type,\n",
    "        \"num_models\": num_models,\n",
    "        \"num_recycles\": num_recycles,\n",
    "        \"recycle_early_stop_tolerance\": recycle_early_stop_tolerance,\n",
    "        \"num_ensemble\": num_ensemble,\n",
    "        \"model_order\": model_order,\n",
    "        \"initial_guess\": initial_guess,\n",
    "        \"keep_existing_results\": keep_existing_results,\n",
    "        \"rank_by\": rank_by,\n",
    "        \"max_seq\": max_seq,\n",
    "        \"max_extra_seq\": max_extra_seq,\n",
    "        \"pair_mode\": pair_mode,\n",
    "        \"pairing_strategy\": pairing_strategy,\n",
    "        \"host_url\": host_url,\n",
    "        \"user_agent\": user_agent,\n",
    "        \"stop_at_score\": stop_at_score,\n",
    "        \"random_seed\": random_seed,\n",
    "        \"num_seeds\": num_seeds,\n",
    "        \"recompile_padding\": recompile_padding,\n",
    "        \"commit\": get_commit(),\n",
    "        \"use_dropout\": use_dropout,\n",
    "        \"use_cluster_profile\": use_cluster_profile,\n",
    "        \"use_fuse\": use_fuse,\n",
    "        \"use_bfloat16\": use_bfloat16,\n",
    "        \"version\": importlib_metadata.version(\"colabfold\"),\n",
    "        \"calc_extra_ptm\": calc_extra_ptm,\n",
    "        \"use_probs_extra\": use_probs_extra,\n",
    "    }\n",
    "    config_out_file = result_dir.joinpath(\"config.json\")\n",
    "    config_out_file.write_text(json.dumps(config, indent=4))\n",
    "    use_env = \"env\" in msa_mode\n",
    "    use_msa = \"mmseqs2\" in msa_mode\n",
    "    use_amber = num_models > 0 and num_relax > 0\n",
    "\n",
    "    bibtex_file = write_bibtex(\n",
    "        model_type if num_models > 0 else \"\", use_msa, use_env, use_templates, use_amber, result_dir\n",
    "    )\n",
    "\n",
    "    if pdb_hit_file is not None:\n",
    "        if local_pdb_path is None:\n",
    "            raise ValueError(\"local_pdb_path is not specified.\")\n",
    "        else:\n",
    "            custom_template_path = result_dir / \"templates\"\n",
    "            put_mmciffiles_into_resultdir(pdb_hit_file, local_pdb_path, custom_template_path)\n",
    "\n",
    "    if custom_template_path is not None:\n",
    "        mk_hhsearch_db(custom_template_path)\n",
    "\n",
    "    pad_len = 0\n",
    "    ranks, metrics = [],[]\n",
    "    first_job = True\n",
    "    job_number = 0\n",
    "    for job_number, (raw_jobname, query_sequence, a3m_lines, _) in enumerate(queries):\n",
    "        if jobname_prefix is not None:\n",
    "            # pad job number based on number of queries\n",
    "            fill = len(str(len(queries)))\n",
    "            jobname = safe_filename(jobname_prefix) + \"_\" + str(job_number).zfill(fill)\n",
    "            job_number += 1\n",
    "        else:\n",
    "            jobname = safe_filename(raw_jobname)\n",
    "\n",
    "        #######################################\n",
    "        # check if job has already finished\n",
    "        #######################################\n",
    "        # In the colab version and with --zip we know we're done when a zip file has been written\n",
    "        result_zip = result_dir.joinpath(jobname).with_suffix(\".result.zip\")\n",
    "        if keep_existing_results and result_zip.is_file():\n",
    "            logger.info(f\"Skipping {jobname} (result.zip)\")\n",
    "            continue\n",
    "        # In the local version we use a marker file\n",
    "        is_done_marker = result_dir.joinpath(jobname + \".done.txt\")\n",
    "        if keep_existing_results and is_done_marker.is_file():\n",
    "            logger.info(f\"Skipping {jobname} (already done)\")\n",
    "            continue\n",
    "\n",
    "        seq_len = len(\"\".join(query_sequence))\n",
    "        logger.info(f\"Query {job_number + 1}/{len(queries)}: {jobname} (length {seq_len})\")\n",
    "\n",
    "        ###########################################\n",
    "        # generate MSA (a3m_lines) and templates\n",
    "        ###########################################\n",
    "        try:\n",
    "            pickled_msa_and_templates = result_dir.joinpath(f\"{jobname}.pickle\")\n",
    "            if pickled_msa_and_templates.is_file():\n",
    "                with open(pickled_msa_and_templates, 'rb') as f:\n",
    "                    (unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features) = pickle.load(f)\n",
    "                logger.info(f\"Loaded {pickled_msa_and_templates}\")\n",
    "\n",
    "            else:\n",
    "                if a3m_lines is None:\n",
    "                    (unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features) \\\n",
    "                    = get_msa_and_templates(jobname, query_sequence, a3m_lines, result_dir, msa_mode, use_templates,\n",
    "                        custom_template_path, pair_mode, pairing_strategy, host_url, user_agent)\n",
    "\n",
    "                elif a3m_lines is not None:\n",
    "                    (unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features) \\\n",
    "                    = unserialize_msa(a3m_lines, query_sequence)\n",
    "                    if use_templates:\n",
    "                        (_, _, _, _, template_features) \\\n",
    "                            = get_msa_and_templates(jobname, query_seqs_unique, unpaired_msa, result_dir, 'single_sequence', use_templates,\n",
    "                                custom_template_path, pair_mode, pairing_strategy, host_url, user_agent)\n",
    "\n",
    "                if num_models == 0:\n",
    "                    with open(pickled_msa_and_templates, 'wb') as f:\n",
    "                        pickle.dump((unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features), f)\n",
    "                    logger.info(f\"Saved {pickled_msa_and_templates}\")\n",
    "\n",
    "            # save a3m\n",
    "            msa = msa_to_str(unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality)\n",
    "            result_dir.joinpath(f\"{jobname}.a3m\").write_text(msa)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Could not get MSA/templates for {jobname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        #######################\n",
    "        # generate features\n",
    "        #######################\n",
    "        try:\n",
    "            (feature_dict, domain_names) \\\n",
    "            = generate_input_feature(query_seqs_unique, query_seqs_cardinality, unpaired_msa, paired_msa,\n",
    "                                     template_features, is_complex, model_type, max_seq=max_seq)\n",
    "\n",
    "            # to allow display of MSA info during colab/chimera run (thanks tomgoddard)\n",
    "            if feature_dict_callback is not None:\n",
    "                feature_dict_callback(feature_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Could not generate input features {jobname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        ###############\n",
    "        # save plots not requiring prediction\n",
    "        ###############\n",
    "\n",
    "        result_files = []\n",
    "\n",
    "        # make msa plot\n",
    "        msa_plot = plot_msa_v2(feature_dict, dpi=dpi)\n",
    "        coverage_png = result_dir.joinpath(f\"{jobname}_coverage.png\")\n",
    "        msa_plot.savefig(str(coverage_png), bbox_inches='tight')\n",
    "        msa_plot.close()\n",
    "        result_files.append(coverage_png)\n",
    "\n",
    "        if use_templates:\n",
    "            templates_file = result_dir.joinpath(f\"{jobname}_template_domain_names.json\")\n",
    "            templates_file.write_text(json.dumps(domain_names))\n",
    "            result_files.append(templates_file)\n",
    "\n",
    "        result_files.append(result_dir.joinpath(jobname + \".a3m\"))\n",
    "        result_files += [bibtex_file, config_out_file]\n",
    "\n",
    "        ######################\n",
    "        # predict structures\n",
    "        ######################\n",
    "        if num_models > 0:\n",
    "            try:\n",
    "                # get list of lengths\n",
    "                query_sequence_len_array = sum([[len(x)] * y\n",
    "                    for x,y in zip(query_seqs_unique, query_seqs_cardinality)],[])\n",
    "\n",
    "                # decide how much to pad (to avoid recompiling)\n",
    "                if seq_len > pad_len:\n",
    "                    if isinstance(recompile_padding, float):\n",
    "                        pad_len = math.ceil(seq_len * recompile_padding)\n",
    "                    else:\n",
    "                        pad_len = seq_len + recompile_padding\n",
    "                    pad_len = min(pad_len, max_len)\n",
    "\n",
    "                # prep model and params\n",
    "                if first_job:\n",
    "                    # if one job input adjust max settings\n",
    "                    if len(queries) == 1 and msa_mode != \"single_sequence\":\n",
    "                        # get number of sequences\n",
    "                        if \"msa_mask\" in feature_dict:\n",
    "                            num_seqs = int(sum(feature_dict[\"msa_mask\"].max(-1) == 1))\n",
    "                        else:\n",
    "                            num_seqs = int(len(feature_dict[\"msa\"]))\n",
    "\n",
    "                        if use_templates: num_seqs += 4\n",
    "\n",
    "                        # adjust max settings\n",
    "                        max_seq = min(num_seqs, max_seq)\n",
    "                        max_extra_seq = max(min(num_seqs - max_seq, max_extra_seq), 1)\n",
    "                        logger.info(f\"Setting max_seq={max_seq}, max_extra_seq={max_extra_seq}\")\n",
    "\n",
    "                    model_runner_and_params = load_models_and_params(\n",
    "                        num_models=num_models,\n",
    "                        use_templates=use_templates,\n",
    "                        num_recycles=num_recycles,\n",
    "                        num_ensemble=num_ensemble,\n",
    "                        model_order=model_order,\n",
    "                        model_type=model_type,\n",
    "                        data_dir=data_dir,\n",
    "                        stop_at_score=stop_at_score,\n",
    "                        rank_by=rank_by,\n",
    "                        use_dropout=use_dropout,\n",
    "                        max_seq=max_seq,\n",
    "                        max_extra_seq=max_extra_seq,\n",
    "                        use_cluster_profile=use_cluster_profile,\n",
    "                        recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
    "                        use_fuse=use_fuse,\n",
    "                        use_bfloat16=use_bfloat16,\n",
    "                        save_all=save_all,\n",
    "                        calc_extra_ptm=calc_extra_ptm\n",
    "                    )\n",
    "                    first_job = False\n",
    "\n",
    "                results = predict_structure(\n",
    "                    prefix=jobname,\n",
    "                    result_dir=result_dir,\n",
    "                    feature_dict=feature_dict,\n",
    "                    is_complex=is_complex,\n",
    "                    use_templates=use_templates,\n",
    "                    sequences_lengths=query_sequence_len_array,\n",
    "                    pad_len=pad_len,\n",
    "                    initial_guess=initial_guess,\n",
    "                    model_type=model_type,\n",
    "                    model_runner_and_params=model_runner_and_params,\n",
    "                    num_relax=num_relax,\n",
    "                    relax_max_iterations=relax_max_iterations,\n",
    "                    relax_tolerance=relax_tolerance,\n",
    "                    relax_stiffness=relax_stiffness,\n",
    "                    relax_max_outer_iterations=relax_max_outer_iterations,\n",
    "                    rank_by=rank_by,\n",
    "                    stop_at_score=stop_at_score,\n",
    "                    prediction_callback=prediction_callback,\n",
    "                    use_gpu_relax=use_gpu_relax,\n",
    "                    random_seed=random_seed,\n",
    "                    num_seeds=num_seeds,\n",
    "                    save_all=save_all,\n",
    "                    save_single_representations=save_single_representations,\n",
    "                    save_pair_representations=save_pair_representations,\n",
    "                    save_recycles=save_recycles,\n",
    "                    calc_extra_ptm=calc_extra_ptm,\n",
    "                    use_probs_extra=use_probs_extra,\n",
    "                )\n",
    "                \n",
    "                result_files += results[\"result_files\"]\n",
    "                ranks.append(results[\"rank\"])\n",
    "                metrics.append(results[\"metric\"])\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                # This normally happens on OOM. TODO: Filter for the specific OOM error message\n",
    "                logger.error(f\"Could not predict {jobname}. Not Enough GPU memory? {e}\")\n",
    "                continue\n",
    "\n",
    "            ###############\n",
    "            # save prediction plots\n",
    "            ###############\n",
    "\n",
    "            # load the scores\n",
    "            scores = []\n",
    "            for r in results[\"rank\"][:5]:\n",
    "                scores_file = result_dir.joinpath(f\"{jobname}_scores_{r}.json\")\n",
    "                with scores_file.open(\"r\") as handle:\n",
    "                    scores.append(json.load(handle))\n",
    "\n",
    "            # write alphafold-db format (pAE)\n",
    "            if \"pae\" in scores[0]:\n",
    "                af_pae_file = result_dir.joinpath(f\"{jobname}_predicted_aligned_error_v1.json\")\n",
    "                af_pae_file.write_text(json.dumps({\n",
    "                    \"predicted_aligned_error\":scores[0][\"pae\"],\n",
    "                    \"max_predicted_aligned_error\":scores[0][\"max_pae\"]}))\n",
    "                result_files.append(af_pae_file)\n",
    "\n",
    "                # make pAE plots\n",
    "                paes_plot = plot_paes([np.asarray(x[\"pae\"]) for x in scores],\n",
    "                    Ls=query_sequence_len_array, dpi=dpi)\n",
    "                pae_png = result_dir.joinpath(f\"{jobname}_pae.png\")\n",
    "                paes_plot.savefig(str(pae_png), bbox_inches='tight')\n",
    "                paes_plot.close()\n",
    "                result_files.append(pae_png)\n",
    "\n",
    "                # make pairwise interface metric plots and chainwise ptm plot\n",
    "                if calc_extra_ptm:\n",
    "                    ext_metric_png = result_dir.joinpath(f\"{jobname}_ext_metrics.png\")\n",
    "                    extra_ptm.plot_chain_pairwise_analysis(scores, fig_path=ext_metric_png)\n",
    "\n",
    "            # make pLDDT plot\n",
    "            plddt_plot = plot_plddts([np.asarray(x[\"plddt\"]) for x in scores],\n",
    "                Ls=query_sequence_len_array, dpi=dpi)\n",
    "            plddt_png = result_dir.joinpath(f\"{jobname}_plddt.png\")\n",
    "            plddt_plot.savefig(str(plddt_png), bbox_inches='tight')\n",
    "            plddt_plot.close()\n",
    "            result_files.append(plddt_png)\n",
    "\n",
    "        if zip_results:\n",
    "            with zipfile.ZipFile(result_zip, \"w\") as result_zip:\n",
    "                for file in result_files:\n",
    "                    result_zip.write(file, arcname=file.name)\n",
    "\n",
    "            # Delete only after the zip was successful, and also not the bibtex and config because we need those again\n",
    "            for file in result_files:\n",
    "                if file != bibtex_file and file != config_out_file:\n",
    "                    file.unlink()\n",
    "        else:\n",
    "            if num_models > 0:\n",
    "                is_done_marker.touch()\n",
    "\n",
    "    logger.info(\"Done\")\n",
    "    return {\"rank\":ranks,\"metric\":metrics}\n",
    "\n",
    "def set_model_type(is_complex: bool, model_type: str) -> str:\n",
    "    # backward-compatibility with old options\n",
    "    old_names = {\n",
    "        \"AlphaFold2-multimer-v1\":\"alphafold2_multimer_v1\",\n",
    "        \"AlphaFold2-multimer-v2\":\"alphafold2_multimer_v2\",\n",
    "        \"AlphaFold2-multimer-v3\":\"alphafold2_multimer_v3\",\n",
    "        \"AlphaFold2-ptm\":        \"alphafold2_ptm\",\n",
    "        \"AlphaFold2\":            \"alphafold2\",\n",
    "        \"DeepFold\":              \"deepfold_v1\",\n",
    "    }\n",
    "    model_type = old_names.get(model_type, model_type)\n",
    "    if model_type == \"auto\":\n",
    "        if is_complex:\n",
    "            model_type = \"alphafold2_multimer_v3\"\n",
    "        else:\n",
    "            model_type = \"alphafold2_ptm\"\n",
    "    return model_type\n",
    "\n",
    "def generate_af3_input(\n",
    "    queries: List[Tuple[str, Union[str, List[str]], Optional[List[str]], Optional[List[Tuple[str, str, int]]]]],\n",
    "    result_dir: Union[str, Path],\n",
    "    msa_mode: str = \"mmseqs2_uniref_env\",\n",
    "    pair_mode: str = \"unpaired_paired\",\n",
    "    pairing_strategy: str = \"greedy\",\n",
    "    use_templates: bool = False,\n",
    "    custom_template_path: str = None,\n",
    "    jobname_prefix: Optional[str] = None,\n",
    "    host_url: str = DEFAULT_API_SERVER, #NOTE: what is this ?\n",
    "    user_agent: str = \"\", #NOTE: what is this ?\n",
    "    # is_complex: bool,\n",
    "    # model_type: str = \"auto\",\n",
    "):\n",
    "    result_dir = Path(result_dir) \n",
    "    result_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    job_number = 0\n",
    "\n",
    "    for job_number, (raw_jobname, query_sequences, a3m_lines, other_molecules) in enumerate(queries):\n",
    "        if jobname_prefix is not None:\n",
    "            # pad job number based on number of queries\n",
    "            fill = len(str(len(queries)))\n",
    "            jobname = safe_filename(jobname_prefix) + \"_\" + str(job_number).zfill(fill)\n",
    "            # job_number += 1 # Why add?\n",
    "        else:\n",
    "            jobname = safe_filename(raw_jobname)\n",
    "\n",
    "        ###########################################\n",
    "        # generate MSA (a3m_lines) and templates\n",
    "        ###########################################\n",
    "        try:\n",
    "            if a3m_lines is None:\n",
    "                (unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features) \\\n",
    "                = get_msa_and_templates(jobname, query_sequences, a3m_lines, result_dir, msa_mode, use_templates,\n",
    "                    custom_template_path, pair_mode, pairing_strategy, host_url, user_agent)\n",
    "\n",
    "            elif a3m_lines is not None:\n",
    "                (unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features) \\\n",
    "                = unserialize_msa(a3m_lines, query_sequences)\n",
    "                # if use_templates:\n",
    "                #     (_, _, _, _, template_features) \\\n",
    "                #         = get_msa_and_templates(jobname, query_seqs_unique, unpaired_msa, result_dir, 'single_sequence', use_templates,\n",
    "                #             custom_template_path, pair_mode, pairing_strategy, host_url, user_agent)\n",
    "\n",
    "            # save json\n",
    "            af3 = AF3Utils(jobname, query_seqs_unique, query_seqs_cardinality, unpaired_msa, paired_msa, other_molecules)\n",
    "            with open(result_dir.joinpath(f\"{jobname}.json\"), \"w\") as f:\n",
    "                f.write(json.dumps(af3.content, indent = 4))\n",
    "                \n",
    "            # save a3m\n",
    "            msa = msa_to_str(unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality)\n",
    "            result_dir.joinpath(f\"{jobname}.a3m\").write_text(msa)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to generate AF3 input json for {jobname}: Could not get MSA/templates. {e}\")\n",
    "            continue\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        \"input\",\n",
    "        default=\"input\",\n",
    "        help=\"One of: 1) directory with FASTA/A3M files, 2) CSV/TSV file, 3) FASTA file or 4) A3M file.\",\n",
    "    )\n",
    "    parser.add_argument(\"results\", help=\"Results output directory.\")\n",
    "\n",
    "    msa_group = parser.add_argument_group(\"MSA arguments\", \"\")\n",
    "    msa_group.add_argument(\n",
    "        \"--msa-only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Query and store MSAs from the MSA server without structure prediction\",\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--msa-mode\",\n",
    "        default=\"mmseqs2_uniref_env\",\n",
    "        choices=[\n",
    "            \"mmseqs2_uniref_env\",\n",
    "            \"mmseqs2_uniref_env_envpair\",\n",
    "            \"mmseqs2_uniref\",\n",
    "            \"single_sequence\",\n",
    "        ],\n",
    "        help=\"Databases to use to create the MSA: UniRef30+Environmental (default), UniRef30 only or None. \"\n",
    "        \"Using an A3M file as input overwrites this option.\",\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--pair-mode\",\n",
    "        help=\"Multimer MSA pairing mode for complex prediction: unpaired MSA only, paired MSA only, both (default).\",\n",
    "        type=str,\n",
    "        default=\"unpaired_paired\",\n",
    "        choices=[\"unpaired\", \"paired\", \"unpaired_paired\"],\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--pair-strategy\",\n",
    "        help=\"How sequences are paired during MSA pairing for complex prediction. \"\n",
    "        \"complete: MSA sequences should only be paired if the same species exists in all MSAs. \"\n",
    "        \"greedy: MSA sequences should only be paired if the same species exists in at least two MSAs. \"\n",
    "        \"Typically, greedy produces better predictions as it results in more paired sequences. \"\n",
    "        \"However, in some cases complete pairing might help, especially if MSAs are already large and can be well paired. \",\n",
    "        type=str,\n",
    "        default=\"greedy\",\n",
    "        choices=[\"complete\", \"greedy\"],\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--templates\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Query PDB templates from the MSA server. \"\n",
    "        'If this parameter is not set, \"--custom-template-path\" and \"--pdb-hit-file\" will not be used. '\n",
    "        \"Warning: This can result in the MSA server being queried with A3M input. \"\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--custom-template-path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Directory with PDB files to provide as custom templates to the predictor. \"\n",
    "        \"No templates will be queried from the MSA server. \"\n",
    "        \"'--templates' argument is also required to enable this.\",\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--pdb-hit-file\",\n",
    "        default=None,\n",
    "        help=\"Path to a BLAST-m8 formatted PDB hit file corresponding to the input A3M file (e.g. pdb70.m8). \"\n",
    "        \"Typically, this parameter should be used for a MSA generated by 'colabfold_search'. \"\n",
    "        \"'--templates' argument is also required to enable this.\",\n",
    "    )\n",
    "    msa_group.add_argument(\n",
    "        \"--local-pdb-path\",\n",
    "        default=None,\n",
    "        help=\"Directory of a local mirror of the PDB mmCIF database (e.g. /path/to/pdb/divided). \"\n",
    "        \"If provided, PDB files from the directory are used for templates specified by '--pdb-hit-file'. \",\n",
    "    )\n",
    "\n",
    "    pred_group = parser.add_argument_group(\"Prediction arguments\", \"\")\n",
    "    pred_group.add_argument(\n",
    "        \"--num-recycle\",\n",
    "        help=\"Number of prediction recycles. \"\n",
    "        \"Increasing recycles can improve the prediction quality but slows down the prediction.\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--recycle-early-stop-tolerance\",\n",
    "        help=\"Specify convergence criteria. \"\n",
    "        \"Run recycles until the distance between recycles is within the given tolerance value.\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--num-ensemble\",\n",
    "        help=\"Number of ensembles. \"\n",
    "        \"The trunk of the network is run multiple times with different random choices for the MSA cluster centers. \"\n",
    "        \"This can result in a better prediction at the cost of longer runtime. \",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--num-seeds\",\n",
    "        help=\"Number of seeds to try. Will iterate from range(random_seed, random_seed+num_seeds). \"\n",
    "        \"This can result in a better/different prediction at the cost of longer runtime. \",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--random-seed\",\n",
    "        help=\"Changing the seed for the random number generator can result in better/different structure predictions.\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--num-models\",\n",
    "        help=\"Number of models to use for structure prediction. \"\n",
    "        \"Reducing the number of models speeds up the prediction but results in lower quality.\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        choices=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--model-type\",\n",
    "        help=\"Predict structure/complex using the given model. \"\n",
    "        'Auto will pick \"alphafold2_ptm\" for structure predictions and \"alphafold2_multimer_v3\" for complexes. '\n",
    "        \"Older versions of the AF2 models are generally worse, however they can sometimes result in better predictions. \"\n",
    "        \"If the model is not already downloaded, it will be automatically downloaded. \",\n",
    "        type=str,\n",
    "        default=\"auto\",\n",
    "        choices=[\n",
    "            \"auto\",\n",
    "            \"alphafold2\",\n",
    "            \"alphafold2_ptm\",\n",
    "            \"alphafold2_multimer_v1\",\n",
    "            \"alphafold2_multimer_v2\",\n",
    "            \"alphafold2_multimer_v3\",\n",
    "            \"deepfold_v1\",\n",
    "        ],\n",
    "    )\n",
    "    pred_group.add_argument(\"--model-order\", default=\"1,2,3,4,5\", type=str)\n",
    "    pred_group.add_argument(\n",
    "        \"--initial-guess\",\n",
    "        nargs=\"?\",\n",
    "        const=True,\n",
    "        help=\"Specify a starting model for the prediction. If the main input file is a PDB format, \"\n",
    "        \"it will be used as the initial guess. Otherwise, you can provide an input file with this flag, \"\n",
    "        \"which will override the main input.\"\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--use-dropout\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Activate dropouts during inference to sample from uncertainty of the models. \"\n",
    "        \"This can result in different predictions and can be (carefully!) used for conformations sampling.\",\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--max-seq\",\n",
    "        help=\"Number of sequence clusters to use. \"\n",
    "        \"This can result in different predictions and can be (carefully!) used for conformations sampling.\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--max-extra-seq\",\n",
    "        help=\"Number of extra sequences to use. \"\n",
    "        \"This can result in different predictions and can be (carefully!) used for conformations sampling.\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--max-msa\",\n",
    "        help=\"Defines: `max-seq:max-extra-seq` number of sequences to use in one go. \"\n",
    "        '\"--max-seq\" and \"--max-extra-seq\" are ignored if this parameter is set.',\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--disable-cluster-profile\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Experimental: For multimer models, disable cluster profiles.\",\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--calc-extra-ptm\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Experimental: calculate pairwise metrics (ipTM and actifpTM), and also chain-wise pTM\",\n",
    "    )\n",
    "    pred_group.add_argument(\n",
    "        \"--no-use-probs-extra\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Experimental: instead of contact probabilities form use binary contacts for extra metrics calculation\",\n",
    "    )\n",
    "    pred_group.add_argument(\"--data\", help=\"Path to AlphaFold2 weights directory.\")\n",
    "\n",
    "    relax_group = parser.add_argument_group(\"Relaxation arguments\", \"\")\n",
    "    relax_group.add_argument(\n",
    "        \"--amber\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable OpenMM/Amber for structure relaxation. \"\n",
    "        \"Can improve the quality of side-chains at a cost of longer runtime. \"\n",
    "    )\n",
    "    relax_group.add_argument(\n",
    "        \"--num-relax\",\n",
    "        help=\"Specify how many of the top ranked structures to relax using OpenMM/Amber. \"\n",
    "        \"Typically, relaxing the top-ranked prediction is enough and speeds up the runtime. \",\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    relax_group.add_argument(\n",
    "        \"--relax-max-iterations\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=\"Maximum number of iterations for the relaxation process. \"\n",
    "        \"AlphaFold2 sets this to unlimited (0), however, we found that this can lead to very long relaxation times for some inputs.\",\n",
    "    )\n",
    "    relax_group.add_argument(\n",
    "        \"--relax-tolerance\",\n",
    "        type=float,\n",
    "        default=2.39,\n",
    "        help=\"Tolerance threshold for relaxation convergence.\",\n",
    "    )\n",
    "    relax_group.add_argument(\n",
    "        \"--relax-stiffness\",\n",
    "        type=float,\n",
    "        default=10.0,\n",
    "        help=\"Stiffness parameter for relaxation.\",\n",
    "    )\n",
    "    relax_group.add_argument(\n",
    "        \"--relax-max-outer-iterations\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"Maximum number of outer iterations for the relaxation process.\",\n",
    "    )\n",
    "    relax_group.add_argument(\n",
    "        \"--use-gpu-relax\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Run OpenMM/Amber on GPU instead of CPU. \"\n",
    "        \"This can significantly speed up the relaxation runtime, however, might lead to compatibility issues with CUDA. \"\n",
    "        \"Unsupported on AMD/ROCM and Apple Silicon.\",\n",
    "    )\n",
    "\n",
    "    output_group = parser.add_argument_group(\"Output arguments\", \"\")\n",
    "    output_group.add_argument(\n",
    "        \"--rank\",\n",
    "        help='Choose metric to rank the \"--num-models\" predicted models.',\n",
    "        type=str,\n",
    "        default=\"auto\",\n",
    "        choices=[\"auto\", \"plddt\", \"ptm\", \"iptm\", \"multimer\"],\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--stop-at-score\",\n",
    "        help=\"Compute models until pLDDT (single chain) or pTM-score (multimer) > threshold is reached. \"\n",
    "        \"This speeds up prediction by running less models for easier queries.\",\n",
    "        type=float,\n",
    "        default=100,\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--jobname-prefix\",\n",
    "        help=\"If set, the jobname will be prefixed with the given string and a running number, instead of the input headers/accession.\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--save-all\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Save all raw outputs from model to a pickle file. \"\n",
    "        \"Useful for downstream use in other models.\"\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--save-recycles\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Save all intermediate predictions at each recycle iteration.\",\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--save-single-representations\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Save the single representation embeddings of all models.\",\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--save-pair-representations\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Save the pair representation embeddings of all models.\",\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--overwrite-existing-results\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Do not recompute results, if a query has already been predicted.\",\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--zip\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Zip all results into one <jobname>.result.zip and delete the original files.\",\n",
    "    )\n",
    "    output_group.add_argument(\n",
    "        \"--sort-queries-by\",\n",
    "        help=\"Sort input queries by: none, length, random. \"\n",
    "        \"Sorting by length speeds up prediction as models are recompiled less often.\",\n",
    "        type=str,\n",
    "        default=\"length\",\n",
    "        choices=[\"none\", \"length\", \"random\"],\n",
    "    )\n",
    "\n",
    "    adv_group = parser.add_argument_group(\n",
    "        \"Advanced arguments\", \"\"\n",
    "    )\n",
    "    adv_group.add_argument(\n",
    "        \"--host-url\",\n",
    "        default=DEFAULT_API_SERVER,\n",
    "        help=\"Which MSA server should be queried. By default, the free public MSA server hosted by the ColabFold team is queried. \"\n",
    "    )\n",
    "    adv_group.add_argument(\n",
    "        \"--disable-unified-memory\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"If you are getting TensorFlow/Jax errors, it might help to disable this.\",\n",
    "    )\n",
    "    adv_group.add_argument(\n",
    "        \"--recompile-padding\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Whenever the input length changes, the model needs to be recompiled. \"\n",
    "        \"We pad sequences by the specified length, so we can e.g., compute sequences from length 100 to 110 without recompiling. \"\n",
    "        \"Individual predictions will become marginally slower due to longer input, \"\n",
    "        \"but overall performance increases due to not recompiling. \"\n",
    "        \"Set to 0 to disable.\",\n",
    "    )\n",
    "\n",
    "    af3_group = parser.add_argument_group(\n",
    "        \"AlphaFold3 arguments\", \"\"\n",
    "    )\n",
    "    af3_group.add_argument(\n",
    "        \"--af3-json\",\n",
    "        help=\"Generate input JSON for AlphaFold3 from the provided FASTA/A3M file.\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if (args.custom_template_path is not None) and (args.pdb_hit_file is not None):\n",
    "        raise RuntimeError(\"Arguments --pdb-hit-file and --custom-template-path cannot be used simultaneously.\")\n",
    "    # disable unified memory\n",
    "    if args.disable_unified_memory:\n",
    "        for k in ENV.keys():\n",
    "            if k in os.environ: del os.environ[k]\n",
    "\n",
    "    setup_logging(Path(args.results).joinpath(\"log.txt\"))\n",
    "\n",
    "    version = importlib_metadata.version(\"colabfold\")\n",
    "    commit = get_commit()\n",
    "    if commit:\n",
    "        version += f\" ({commit})\"\n",
    "\n",
    "    logger.info(f\"Running colabfold {version}\")\n",
    "\n",
    "    data_dir = Path(args.data or default_data_dir)\n",
    "\n",
    "    queries, is_complex = get_queries(args.input, args.sort_queries_by)\n",
    "\n",
    "    model_type = set_model_type(is_complex, args.model_type)\n",
    "\n",
    "    # use pdb or cif input as initial guess\n",
    "    if args.initial_guess is not None:\n",
    "        if isinstance(args.initial_guess, str) and Path(args.initial_guess).suffix in (\".pdb\", \".cif\"):\n",
    "            initial_guess = args.initial_guess\n",
    "        elif Path(args.input).suffix in (\".pdb\", \".cif\"):\n",
    "            initial_guess = args.input\n",
    "        else:\n",
    "            raise ValueError(\"Provide PDB or CIF file for initial guess.\")\n",
    "    else:\n",
    "        initial_guess = None\n",
    "\n",
    "    if args.msa_only:\n",
    "        args.num_models = 0\n",
    "\n",
    "    if args.num_models > 0:\n",
    "        download_alphafold_params(model_type, data_dir)\n",
    "\n",
    "    if args.msa_mode != \"single_sequence\" and not args.templates:\n",
    "        uses_api = any((query[2] is None for query in queries))\n",
    "        if uses_api and args.host_url == DEFAULT_API_SERVER:\n",
    "            print(ACCEPT_DEFAULT_TERMS, file=sys.stderr)\n",
    "\n",
    "    model_order = [int(i) for i in args.model_order.split(\",\")]\n",
    "\n",
    "    assert args.recompile_padding >= 0, \"Can't apply negative padding\"\n",
    "\n",
    "    # backward compatibility\n",
    "    if args.amber and args.num_relax == 0:\n",
    "        args.num_relax = args.num_models * args.num_seeds\n",
    "\n",
    "    # added for actifptm calculation\n",
    "    use_probs_extra = False if args.no_use_probs_extra else True\n",
    "\n",
    "    user_agent = f\"colabfold/{version}\"\n",
    "\n",
    "    if args.af3_json:\n",
    "        generate_af3_input(\n",
    "            queries=queries,\n",
    "            result_dir=args.results,\n",
    "            msa_mode=args.msa_mode,\n",
    "            pair_mode=args.pair_mode,\n",
    "            pairing_strategy=args.pair_strategy,\n",
    "            use_templates=args.templates,\n",
    "            custom_template_path=args.custom_template_path,\n",
    "            jobname_prefix=args.jobname_prefix,\n",
    "            host_url=args.host_url,\n",
    "            user_agent=user_agent,\n",
    "            # extra_molecules=extra_molecules,\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    run(\n",
    "        queries=queries,\n",
    "        result_dir=args.results,\n",
    "        use_templates=args.templates,\n",
    "        custom_template_path=args.custom_template_path,\n",
    "        num_relax=args.num_relax,\n",
    "        relax_max_iterations=args.relax_max_iterations,\n",
    "        relax_tolerance=args.relax_tolerance,\n",
    "        relax_stiffness=args.relax_stiffness,\n",
    "        relax_max_outer_iterations=args.relax_max_outer_iterations,\n",
    "        msa_mode=args.msa_mode,\n",
    "        model_type=model_type,\n",
    "        num_models=args.num_models,\n",
    "        num_recycles=args.num_recycle,\n",
    "        recycle_early_stop_tolerance=args.recycle_early_stop_tolerance,\n",
    "        num_ensemble=args.num_ensemble,\n",
    "        model_order=model_order,\n",
    "        initial_guess=initial_guess,\n",
    "        is_complex=is_complex,\n",
    "        keep_existing_results=not args.overwrite_existing_results,\n",
    "        rank_by=args.rank,\n",
    "        pair_mode=args.pair_mode,\n",
    "        pairing_strategy=args.pair_strategy,\n",
    "        data_dir=data_dir,\n",
    "        host_url=args.host_url,\n",
    "        user_agent=user_agent,\n",
    "        random_seed=args.random_seed,\n",
    "        num_seeds=args.num_seeds,\n",
    "        stop_at_score=args.stop_at_score,\n",
    "        recompile_padding=args.recompile_padding,\n",
    "        zip_results=args.zip,\n",
    "        save_single_representations=args.save_single_representations,\n",
    "        save_pair_representations=args.save_pair_representations,\n",
    "        use_dropout=args.use_dropout,\n",
    "        max_seq=args.max_seq,\n",
    "        max_extra_seq=args.max_extra_seq,\n",
    "        max_msa=args.max_msa,\n",
    "        pdb_hit_file=args.pdb_hit_file,\n",
    "        local_pdb_path=args.local_pdb_path,\n",
    "        use_cluster_profile=not args.disable_cluster_profile,\n",
    "        use_gpu_relax = args.use_gpu_relax,\n",
    "        jobname_prefix=args.jobname_prefix,\n",
    "        save_all=args.save_all,\n",
    "        save_recycles=args.save_recycles,\n",
    "        calc_extra_ptm=args.calc_extra_ptm,\n",
    "        use_probs_extra=use_probs_extra,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
